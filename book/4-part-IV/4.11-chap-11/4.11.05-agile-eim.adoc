==== Agile information management

===== Software versus data
[quote, Dan McCrory]
Services and Applications can have their own Gravity, but Data is the most massive and dense, therefore it has the most gravity. Data if large enough can be virtually impossible to move.
<<McCrory2010>>

Enterprise information management (including data management) has had a contentious relationship to Agile methods. There are inherent differences in perspective between those who focus on implementing software, versus those who are concerned with data and information, especially at scale.

The tendency among software engineers is to focus on the conceptual aspects of software, without concern for the physical issues of computing. (Logical versus physical is arguably the fundamental distinction at the heart of the xref:continuous-delivery[development vs operations divide].)

As we have previously discussed, xref:refactoring[refactoring] is one of the critical practices of Agile development, keeping systems flexible and current when used appropriately. The simple act of turning one overly-large software class or module into two or three more cohesive elements is performed every day by developers around the world, as they strive for clean, well engineered code.

DevOps and Agile techniques can be applied to databases; in fact there are important technical books such as Refactoring Databases by Ambler and Sadalage <<Ambler2006>>. With smaller systems, there is little reason to avoid ongoing refactoring of data along with code.

Infrastructure as Code techniques apply. Database artifacts (e.g. SQL scripts, export/import scripts, etc.) must be under version control and should leverage continuous integration techniques. Test-driven development can apply equally well to database-related development.

But when data reaches a certain scale, its concerns start to become priorities. The bandwidth of FedEx is still greater than that of the Internet (<<Munroe2013>>). That is to say, it is more effective and efficient, past a certain scale, to physically move data by moving the hard drives around, than to copy the data. The reasons are well understood and trace back to fundamental laws of physics and information first understood by Claude Shannon <<Shannon1949>>. The concept of "data gravity" (quote above) seems consistent with Shannon's work.

Notice that these physical limitations apply not just to simple movement of data, but also to complex xref:refactoring[refactoring]. It's one thing to refactor code -- even the SQL defining a table. Breaking an existing, overly-large database table and its contents into several more specialized tables is a different problem, if data is large. Data, in the sense understood by digital and IT professionals, is *persistent*. It exists as physical indications of state in the physical world: whether knotted ropes, clay tablets, or electromagnetic state.

If you are transforming 3 billion rows in a table, each row of data has to be processed. The data might take hours or days to be restructured, and what of the business needs in the meantime? These kinds of situations often have messy and risky solutions, that cannot easily be "rolled back":

. A copy might be made for the restructuring, leaving the original table in place.
. When the large restructuring operation (perhaps taking hours or days) is completed, and new code is released, a careful conversion exercise must identify the records that changed while the large restructuring occurred.
. These records must then go through the conversion process AGAIN and be updated in the new data structure. They MUST replace the older data that was initially converted in step 1.

ifdef::collaborator-draft[]
 back/forward ref to change management - these are risky changes
endif::collaborator-draft[]


All in all, this is an error-prone process, requiring careful auditing and cross-checking to mitigate the risk of information loss. It can and should be automated to the maximum degree possible.

Modern web-scale architectures are often built to accommodate rolling upgrades in a more efficient manner (see <<Limoncelli2014>> Chapter 11, "Upgrading Live Services"). Perhaps the data can be transformed on a "if changed, then transform" basis, or via other techniques. Nevertheless, schema changes can still be problematic. Some system outage may be unavoidable, especially in systems with strong transactional needs for complete integrity. (See earlier discussion of xref:CAP-theorem[CAP theorem].)

Because of these issues, there will always be some contention between the software and information management perspectives. Software developers, especially those schooled in Agile practices, tend to rely on heuristics such as "do the simplest thing that could possibly work." Such a heuristic might lead to a preference for a simpler data structure, that will not hold up past a certain scale.

An information professional, when faced with the problem of restructuring the now-massive contents of the data structure, might well say "Why did you build it that way? Couldn't you have thought a little more about this?"

In fact, data management capabilities often have sought to intervene with developers, sometimes placing procedural requirements upon the development teams when database services were required. This approach is seen in xref:Agile-history[waterfall] development and when database teams are organized xref:product-v-function[functionally].

We saw the classic model xref:data-modeling[earlier] in this chapter: in turn, define

. Conceptual data model
. Logical data model
. Physical schema

as a sequential process.

But organizations pressed for time often go straight to defining physical schemas. And indeed, if the xref:cost-of-delay[Cost of Delay] is steep, this behavior will *not* change.

The only reason to invest in richer understanding of information (e.g. conceptual and logical modeling), or more robust and proven data structures, is if the benefits outweigh the costs. Data and records management justifies itself when:

* Systems are easier to adapt, because they are well understood and the data structures are flexible
* Occurrence, costs, and risks of data refactoring are reduced
* Systems are easier to use, because the meaning of their information is documented for the end consumer (reducing support costs and operational risks)
* Data redundancy is lessened, saving storage and management attention
* Data and records-related risks (security, regulatory, liability) are mitigated through better data management.

Again, back to our xref:emergence-model[emergence model]. By the time you are an enterprise, faced with the full range of Governance, Risk, Security, and Compliance concerns, you likely need at least *some* of the benefits promised by Enterprise Information Management. But data management, like any xref:product-v-function[functional] domain, can become an end in itself, losing sight of the reasons for its existence.

===== Next generation practices in information management

====== Cross-functional teams
The value of cross-functional teams was discussed at length in xref:product-v-function[chapter 7]. This applies to including data specialists as team members. This practice alone can reduce many data management issues.

====== Domain-driven design
[quote, Martin Fowler]
As you try to model a larger domain, it gets progressively harder to build a single unified model <<Fowler2014>>.

The relational database, with its fast performance and integrated schemas allowing data to be joined with great flexibility, has fundamentally defined the worldview of data managers for decades now.

This model arguably reached its peak in highly-scaled mainframe systems, where all corporate data might be available for instantaneous query and operational use.

However, when data is shared for many purposes, it becomes very difficult to change. The analysis process starts to lengthen, and xref:cost-of-delay[Cost of Delay] increases for new or enhanced systems.

This started to become a real problem right around the time that cheaper distributed systems became available in the market. Traditonal data managers of large scale mainframe database systems continued to have a perspective that "everything can fit in my database." But the demand for new digital product outstripped their capacity to analyze and incorporate the new data.

The information management landscape became fragmented. One response was the enterprise conceptual data model. The idea was to create a sort of "master schema" for the enterprise, that would define all the major concepts in an unambiguous way.

However, attempting to establish such a model can run into difficulties getting agreement on definitions. (See the xref:ontology-problem[ontology problem] above.) Seeking such agreement again can impose cost of delay, if gaining agreement is required for the system. And if gaining agreement is optional, then why is agreement being sought? The risk is that the data architect becomes "ivory tower."

NOTE: In fact, there are theoretical concerns at the heart of philosophy with attempting to formulate universal ontologies. They are beyond the scope of this text but if you are interested, start by researching _semiotics_ and _postmodernism_. Such concerns may seem academic, but we see their consequences in the practical difficulty of creating universal data models.

A pragmatic response to these difficulties is represented in the Martin Fowler quote above. Fowler recommends the practice of domain-driven design, which accepts the fact that "Different groups of people will use subtly different vocabularies in different parts of a large organization" <<Fowler2014>> and quotes Eric Evans that "total unification of the domain model for a large system will not be feasible or cost-effective" <<Evans2004>>.

Instead, there are various techniques for relating these contexts, beyond the scope of this book. (See <<Evans2004>>.) Some will argue for the use of microservices, but data always wants to be recombined, so microservices have limitations as a solution for the problems of information management.

And, before you completely adopt a domain-driven design approach, be certain you understand the consequences for data governance and records management. Human resources records are human resources records and must be handled appropriately. Regulators and courts will not accept "domain driven design" as a defense for non-compliance.

anchor:schema-less[]

====== Generic structures and inferred schemas

Schema development - the creation of detailed logical and physical data and/or object models - is time consuming and requires certain skills. Sometimes, application developers try to use highly generic structures in the database. Relational databases and their administrators prefer distinct tables for Customer, Invoice, and Product, with specifically identified attributes such as Invoice Date. Periodically, developers might call up the database administrator and have a conversation like this (only slightly exaggerated):

"I need some tables."

"OK, what are their descriptions?"

"Just give me 20 or so tables with 50 columns each. Call them Table1 through Table20 and Column1 through Column50. Make the columns 5000-character strings, that way they can hold anything."

"Ummm... You need to model the data. The tables and columns have to have names we can understand."

"Why? I'll have all that in the code."

These conversations usually would result in an unsatisfied developer and a DBA further convinced that developers just didn't understand data.

A relational database, for example, will not perform well at scale using such an approach. Also, there is nothing preventing the developer from mixing data in the tables, using the same columns to store different things.

This might not be a problem for smaller organizations, but in organizations with compliance requirements, knowing with confidence what data is stored where is not optional.

This does not mean that the developer was completely off track. New approaches to data warehousing use generic schemas similar to what the developer was requesting. Speed of indexing and proper records management, can be solved in a variety of ways.

Recently, the concept of the "data lake" has gained traction.

Some data has always been a challenge to adapt into traditional, rigid, structured relational databases. Modern “web-scale” companies such as Google have pioneered new, less structured data management tools and techniques.

The data lake integrates data from a large variety of sources, but does not seek to integrate them into one master structure (also known as a schema) when they are imported. Instead, the data lake requires the analysts to specify a structure when the data is extracted for analysis. This is known as "schema-on-read," in contrast to the traditional model of "schema on write."

Data lakes, and the platforms that support them (such as Hadoop) were originally created high volume web data such as generated by Google. There was no way that traditional relational databases could scale to these needs, and the data was not transactional – it was harvested and in general never updated afterwards.

This is an increasingly important kind of workload for  digital organizations. As the Internet of Things takes shape, and digital devices are embedded throughout daily experiences, high-volume, adaptable datastores (such as data lakes) will continue to spread.

Because log formats change, and the collaboration data is semi-structured, analytics will likely be better served with a “schema on read” approach. However, this means that the operational analysis is significant development. Simplifying the load logic only defers the complexity. The data lake analyst must have a thorough understanding of the various event formats and other data brought into the lake, in order to write the operational analysis query.

"Schema on read" still may be a more efficient approach however. Extensive schema development done up front may be invalidated by actual data use, and heavyweight approaches are not as compatible with fast feedback. (Data services are also a form of product development and therefore fast xref:feedback[feedback] on their use is beneficial; the problem again is one of data gravity. Fast feedback works in software because code is orders of magnitude easier to change.)

Schema inference at the most general shades into _ontology mining_. In ontology mining, data (usually text-heavy) is analyzed by algorithms to derive the data model. If one reads a textbook about the retail business, one might easily infer that there are concepts such as "store," "customer," "warehouse," and "supplier." Information technology has reached a point where such analysis itself can be automated, to a degree. Certain analytics systems have the ability to display an inferred table structure derived from unstructured or semi-structured data. This is an active area of research, development, and product innovation.

 cite

The challenge is that data still needs to be tagged and identified; *regulatory concerns do not go away* just because one is using a NoSQL database.

For further information and the current state of industry practice on these questions, see the professional associations at the end of this chapter.

====== Append-only to the rescue?
Another technique that is changing the data management landscape is the concept of append-only. Traditional databases *change* values; for example, if you change "1004 Oak Av." to "2010 Elm St." in an address field, the old value is (in general) *gone*, unless you have specifically engineered the system to preserve it.

A common approach is the idea of "audited" or "effective-dated" fields, which have existed for decades. In an effective-dated approach, the "change" to the address actually looks like this in the database:
[cols="3*", options="header"]
|====
|Street address|From|To
|1004 Oak Av.|12/1/1995|9/1/2016
|2010 Elm St.|9/2/2016|Present
|====
Determining the correct address requires a query on the To date field. (This is only an example, there are many ways of solving the problem.)

In this approach, data accumulates and is not deleted. (Capacity problems can of course result.) Append-only takes the idea of effective dating and applies it across the entire database. No values are ever changed, they are only superseded by further appends.

This is a powerful technique, especially as storage costs go down. It can be combined with the data lake to create systems of great flexibility. But there are no silver bullets. Suppose that a distributed system has sacrificed consistency for availability and partition-tolerance (see xref:CAP-theorem[CAP theorem]). In that case, the system may wind up with data such as:

[cols="3*", options="header"]
|====
|Street address|From|To
|1004 Oak Av.|12/1/1995|9/1/2016
|2010 Elm St.|9/2/2016|Present
|574 Maple St.|9/2/2016|Present
|====

This is now a xref:data-quality[data quality] issue, requiring after-the-fact exception analysis and remediation.

Append-only complements architectural and programming language trends towards _immutability_.

anchor:test-data-mgmt[]

====== Test data
[quote, Puppet Labs/DevOps Research and Assessment, 2016 State of DevOps Report]
...when teams have adequate test data to run automated tests, and can create that data on demand, they see better IT performance

A non-obvious and non-trivial problem at the intersection of Enterprise Information Management and DevOps is test data management.

What is test data management?

Suppose you are a developer working on a data-intensive system, one that (for example) handles millions of customer or supply chain records.

Your code needs to support a wide variety of data inputs and outputs. At first, you just entered a few test names and addresses, like "Mickey Mouse" or "Bugs Bunny, 123 Carrot Way, Albuquerque, New Mexico 10001."

But this nonsensical data quickly was shown to not work. For example, if you are testing integration with an address-scrubbing service, you will get an error with an address in New Mexico that shows a ZIP code of 10001. (Actually, the nonsensical data is useful in testing that particular error scenario. But that is only one of many error scenarios.)

Based on hearing anecdotal concerns, the authors of the _2016 State of DevOps_ report examined test data management practices and found that they correlated postively with "better IT performance, lower change failure rates, and lower levels of deployment pain and rework" <<Forsgren2016>>, p. 29. In particular, the report suggests that test data be minimized and created from a blank slate wherever possible.

Taking data from production systems as a basis for testing is also frequently done. However, such data must be sanitized -- sensitive information such as social security number must be removed. This can be done automatically, but then such automation must itself be developed and maintained, and the extensive production data set may (in effect) be driving a large amount of non-value-add testing.

In general, test data management techniques will vary greatly by application and problem domain. The primary recommendation here is to invest in solving the problem, understanding that up-front investments in automation will pay off. The high-performing product team will have to solve the "how" of doing it appropriately for their particular situation.


====== Non-invasive data governance (Seiner?)
