
anchor:analytics[]

==== Analytics


ifdef::instructor-ed[]
****
Instructor's note:

This is a brief discussion, because many aspects of big data, analytics, and data science are specific forms of product development. This book for example has avoided detailed discussion of programming languages or current vendor products. Similarly, the intent here is to explore the background and management implications of big data, analytics, and data science, not to discuss specific techniques or products in any detail.

****
endif::instructor-ed[]

===== Analytics in context

[quote, Tom Davenport and Jeanne Harris, Competing on Analytics]
...analytical competitors wring every last drop of value from business processes and key decisions.

One important aspect of digital product development is data analytics. Analysis of organizational records has always been a part of any concern large enough to have formal records management. The word for this originally was simply "reporting." A set of files or ledgers would be provided to one or more clerks, who would manually review them and extract the needed figures.

.Reporting and analytics, old style footnote:[_Image credit https://www.flickr.com/photos/seattlemunicipalarchives/3347281230, commercial use permitted_]
image::images/4_11-reportingOldStyle.png[1900s clerk with adding machine, 300, 200, float="left"]

We have touched on reporting previously, in our discussion of xref:metrics-KPIs[metrics]. This is a more detailed examination.

The compilation of data from physical sources, and its analysis for the purposes of organizational strategy, was distinct from the day to day creation and use of the data. The clerk who attended to the customer and updated their account records, had a different role than the clerk who added up the figures across dozens or hundreds of accounts for the annual corporate report.

What do we mean by the words "analysis" or "analytics" in this older context? Just compiling totals and averages was expensive and time-consuming. Cross-tabulating data (e.g. to understand sales by region) was even more so.

As information became more and more automated, the field of "Decision Support" (and its academic partner "Decision Sciences") emerged. The power of extensively computerized information, that could support more and more ambitious forms of analysis, gave rise to the concept and practice of "Data Warehousing" <<Inmon1992>>.

A robust profession and set of practices emerged around data warehousing and analytics. And as infrastructure became more powerful and storage less expensive, the idea of full-lifecycle or closed-loop analytics originated.

When analyzing data is costly and slow, the data analysis can only affect large, long-cycle decisions. It is not a form of fast feedback. The annual report may drive next year's product portfolio investment decisions, but it cannot drive the day to day behavior of sales, marketing, and customer service:

anchor:analytics-context[]

.Strategic analytics
image::images/4_11-oldCycle.png[old analytics cycle, 500]

However, as analysis becomes faster and faster, it can inform operational decisions.

.Opeational analytics (closed-loop)
image::images/4_11-newCycle.png[new analytics cycle, 500]

And, for certain applications (such as an online traffic management application on your smartphone), analytics is such a fundamental part of the application that it becomes operational. Such pervasive use of analytics is one of the hallmarks of digital transformation.

===== Data warehousing and business intelligence
[quote, Data Management Body of Knowledge]
A Data Warehouse (DW) is a combination of two primary components. The first is an integrated decision support database. The second is the related software programs used to collect, cleanse, transform, and store data from a variety of operational and external sources...Data warehousing is a technology solution supporting Business Intelligence (BI).

[quote, Paul Westerman, Data Warehousing: Using the Wal-Mart Model]
The reason to build a data warehouse is to have the ability to make better decisions faster based on information using current and historic data.

The vision of an integrated data warehouse for decision support is compelling and has provided enough value to support an industry sector of specialized hardware, software, training, and consulting. It can be seen as a common architectural pattern, in which disparate data is aggregated and consolidated for purposes of analysis, reporting, and for feedback into strategy, tactics, and operational concerns.

anchor:DW-BI-illustrated[]

Here is an illustration of an data warehousing/business intelligence (DW/BI) implementation pattern:

.Data warehousing/business intelligence architecture
image::images/4_11-DW-BI.png[DW/BI architecture, 600]

The above diagram expands on the above xref:analytics-context[contextual diagrams], showing the major business areas (Sales etc) as data sources. (In a large organization this might be dozens or hundreds of source systems.) These systems feed a "data services layer" that both aggregates data for analytics, as well as providing direct services such as data cleansing and master data management.

It's important to understand that in terms of this book's emphasis on product-centric development that *the data services layer itself is an internal product.* Some might call it more of a xref:feature-v-component[component than a feature], but it is intended in any case as a general-purpose platform that can support a wide variety of use cases.

"Factoring out" data services in this way may or may not be optimal for any given organization, depending on maturity, business objectives, and a variety of other concerns. However, at scale the skills and practices do become specialized, and so it's anticipated we'll continue to see implementation strategies similar to the above figure.

Notice also that the data services layer is not solely for analytics; it also supports direct operational services.

Here are discussions of the diagrams's various elements:

====== Operational applications
These are the source systems that provide the data and require data services.

====== Quality analysis
This is the capability to analyze data for consistency, integrity, and conformity with expectations, and to track associated metrics over time. (See xref:data-quality[data quality].)

====== Extraction and archiving
As data storage has become less expensive, maintaining a historical record of data extracts in original format is seen more often in data warehousing. (This may use a schema-less data lake for implementation.)

====== Master data reconciliation
When master data exists in diverse locations (e.g. in multiple xref:system-of-record[Systems of Record]) the ability to reconcile and define the true or "golden" master may be required. This is useful directly to operational systems, as an online service (e.g. postal service address verification), and is also important when populating the data warehouse or mart. Master data includes reference data, and in the data warehousing environment may be the basis for "dimensions," a technical term for the ways data can be categorized for analytic purposes (e.g. retail categorizes sales by time, region, and product line). Maintaining a history of dimensions is a challenging topic; search on the "slowly changing dimension" problem for further information.

====== Metadata
Commonly understood as "data about data," we have xref:commit-as-metadata[previously] encountered the concept of metadata and will further discuss it in the next chapter section.

====== Transformation and load
Converting data to a consistent and normalized form has been the basis of enterprise data warehousing since it was first conceived. (We will discuss the xref:schema-less[schema-less] data lake approach in the next chapter section.) A broad market segment of "Extract, Transform, Load" (ETL) tooling exists to support this need.

====== Sourcing and archiving
This represents the physical data store required for the extraction and archiving capability. Again, it may be implemented as a schema-less data lake, or as a traditional relational structure.

====== Integrated data warehouse
The integrated or enterprise data warehouse is the classic, normalized, often massive, historical data store envisioned originally by Bill Inmon <<Inmon1992>>. While the development effort in creating fully normalized data warehouses has limited them, they nevertheless are important, valuable, and frequently encountered in larger organizations.

====== Schema-less lake
A newer form of data aggregation is seen in the xref:schema-less[schema-less] lake. As discussed in the next chapter section, schema-less approaches accept data in native formats and defer the hard question of normalizing the data to to the reporting and analysis stage.

====== Mart(s)
The integrated data warehouse is intended to provide a consistent and universal platform across the enterprise. The data mart on the other hand is usually seen as specific to a particular organization or problem.

====== Statistics
Statistical analysis of the aggregated and cleansed data is a common use case, often performed using commercial software or the R programming language.

====== Machine learning
Machine learning is broadly defined as "a field of study that gives computers the ability to learn without being explicitly programmed." [Arthur Samuel as quoted in Simon, waiting on book]. Machine learning allows computers to develop and improve algorithmic models for making predictions or decisions. Spam filters that "learn" are a good example.

====== Visualization
Representing complex information effectively so that humans can understand it and derive value is itself a challenging topic. Many graphical forms have been developed to communicate various aspects of data. See for example the open source visualization library https://d3js.org/[D3.js]

====== Ontology and inference
This includes text mining and analytics, and also the ability to infer meaning from unstructured data sets. More in the next chapter section discussion on xref:schema-less[schema-less].

====== Agile methods meet DW/BI

Data infrastructure, like any complex systems development effort, is most effective and least risky when undertaken iteratively and incrementally. An organization's analysis needs will change unpredictably over time and so a fast feedback loop of testing and learning is essential.

The enterprise data warehouse can support a wide variety of analysis objectives flexibly. Its challenge has always been the lead time required to develop the data structures and ETL logic. This will be discussed further in the next chapter section.
