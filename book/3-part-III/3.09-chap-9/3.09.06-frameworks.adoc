
anchor:frameworks[]

==== Industry frameworks

We will now turn to a critical examination of the IT management frameworks. While there is much of value in them, there are many places where they may lead you into the "defined process" trap (assuming that variation is the enemy), and they may not provide enough support for the alternative approach of xref:empirical-process-control[empirical process control].

===== Defining frameworks

NOTE: There are other usages of the term "framework," especially in terms of software frameworks. Process and management frameworks are non-technical.

So, what is a "framework?"

The term "framework," in the context of business process, is used for comprehensive and systematic representations of a major business area's activities. In general, an industry framework is a structured artifact that seeks to articulate a professional consensus regarding a domain of practice. The intent is usually that the guidance be mutually exclusive and collectively exhaustive within the domain, so that persons knowledgeable in the framework have a broad understanding of domain concerns.

The first goal of any framework, for a given conceptual space, is to provide a "map" of its components and their relationships. Doing this serves a variety of goals:

* Develop and support professional consensus on the business area
* Support training and orientation of professionals new to the area (or its finer points)
* Support governance and control activities related to the area (more on this in Chapter 10)

Many frameworks have emerged in the IT space, with broader and narrower domains of concern. Some are owned by non-profit standards bodies; others are commercial. We will focus on five in this book. In roughly chronological order, they are:

* CMMI (Capability Maturity Model-Integrated)
* ITIL (originally the Information Technology Infrastructure Library)
* PMBOK (The Project Management Body of Knowledge)
* COBIT (aka Control Objectives for Information Technology)
* TOGAF (The Open Group Architecture Framework)

The frameworks are summarized in the xref:framework-summaries[appendix].

===== Observations on the frameworks

In terms of the new digital delivery approaches, there are a number of issues and concerns with the frameworks:

* The fallacy of statistical process control
* Local optimization temptation
* Lack of execution model
* Proliferation of secondary artifacts, compounded by batch orientation
* Confusion of process definition

anchor:problem-statisical-process[]

====== The problem of statistical process control

CMM author Watts Humphrey's original vision was to apply full statistical process control to the software process. As he stated at the time:

_Dr. W. E. Deming, in his work with the Japanese after World War II, applied the concepts of statistical process control to many of their industries. While there are important differences, these concepts are just as applicable to software as they are to producing consumer goods like cameras, television sets, or auto mobiles._ (cite:[Humphrey1989], p. 3)

The overall CMM/CMMI idea (in the well-known staged model) is that a process cannot be improved and optimized until it is fully under control. Perhaps well-defined industrial processes should not be optimized until they are fully "managed." However, as we discussed in the previous section,  process control theorists see creative, knowledge-intensive processes as requiring xref:empirical-process-control[empirical control]. SPC applied to software has therefore been criticized as inappropriate cite:[Raczynski2008].

In CMM terms, empirical process control starts by measuring and immediately optimizing (adjusting). To restate the Martin Fowler quote from the last section: "a process can still be controlled even if it can't be defined."cite:[Schwaber2002] They need not -- and *cannot* -- be fully defined. One of the most questionable aspects of CMMI therefore, is its implication that process optimization is _something only done at the highest levels of maturity_.

*In short, the CMMI staged model encourages the thought that process improvement (optimization) only  is possible at Level 5. Many companies implementing CMMI stages however will pragmatically say "Maybe we only need to get to level 3." This implies that they define and manage their processes, but never improve them.*

This runs against much current thinking and practice, especially that deriving from Lean philosophy, in which processes are seen as always under improvement. (See discussion of xref:Toyota-Kata[Toyota Kata].) All definition, measurement, and control must serve that end.

The CMMI has evolved since Humphrey's initial vision, but between its  mis-applicaton of statistical process control, and the idea that that process optimization is only relevant at the highest maturity, it is (in the view of this author) badly out of step with current digital trends.

The other frameworks do not embrace statistical process control to the same extent as the CMMI. PMBOK suggests that "control charts may also be used to monitor cost and schedule variances, volume, and frequency of scope changes, or other management results to help determine if the project management processes are in control" (cite:[PMI2013], Kindle Locations 4108-4109). This also contradicts the insights of empirical process control, unless the project were also a fully defined process -- unlikely from a process control perspective.

====== Local optimization temptation
[quote, Eli Goldratt, The Goal]
We must not seek to optimize every resource in the system … A system of local optimums is not an optimum system at all; it is a very inefficient system.

IT capability frameworks can be harmful if they lead to fragmentation of improvement effort and lack of focus on the flow of IT value.

The digital delivery system at scale is a complex socio-technical system, including people, process, and technology. Frameworks help in understanding it, by breaking it down into component parts in various ways. This is all well and good, but the danger of *reductionism* emerges.

NOTE: There are various definitions of "reductionism." This discussion reflects one of the more basic versions.

A reductionist view implies that a system is nothing but the sum of its parts. Therefore, if each of the parts is attended to, the system will also function well.

This can lead to a compulsive desire to do "all" of a framework. If ITIL calls for 25 processes, then a large, mature organization by definition should be good at all of them. But the 25 processes (and dozens more sub-processes and activities) called for by ITIL, or the 32 called for by COBIT, are somewhat arbitrary divisions. They overlap with each other.

Furthermore, there are many digital organizations that do not use the full ITIL or COBIT process portfolio and yet deliver value as well as organizations that do use ITIL.

This temptation for local, process-level optimization runs counter to core principles of Lean and Systems Thinking. Many management thinkers, including W.E. Deming, Eli Goldratt, and others have emphasized the dangers of local optimization, and the need for taking a systems view.

As this book's structure suggests, delivering IT value requires different approaches at different scales. There is recognition of this among framework practitioners; however, the frameworks themselves provide insufficient guidance on how they scale up and down.

anchor:lack-execution-model[]

====== Lack of execution model
It is also questionable whether even the largest actual IT organizations on the planet could fully implement the frameworks. Specifying too many interacting processes has its own complications.

Consider: Both ITIL and COBIT devote considerable time to documenting possible process inputs and outputs. As a part of every process definition, ITIL has a section entitled "Triggers, inputs, outputs, and interfaces." The Service Level Management Process (cite:[TSO2011b], pp 120-122) for example, lists:

* 7 triggers (e.g. "service breaches")
* 10 inputs (e.g. "customer feedback")
* 10 outputs (e.g. "reports on OLAs")
* 7 interfaces (e.g. "Supplier management")

COBIT similarly details process inputs and outputs. In the Enabling Processes guidance, each management practice suggests inputs and outputs. For example, the APO08 process "Manage Relationships" has an activity of "Provide input to the continual improvement of services," with

* 6 inputs
* 2 outputs

But processes do not run themselves. These process inputs and outputs require staff attention. They often imply xref:queuing[queues] and therefore Work in Process, often invisible. They impose demand on the system and each handoff represents transactional friction. Some handoffs may be implemented within the context of an IT management suite; others may require procedural standards, which themselves need to be created and maintained. The industry currently lacks understanding of how feasible such fully elaborated frameworks are, in terms of the time, effort, and organizational structure they imply.

We have discussed the issue of overburden previously. Too many organizations have contending execution models, where projects, processes, and miscellaneous work all compete for people's attention. In such environments, the overburden and wasteful xref:multi-tasking[multi-tasking] can reach crisis levels. With ITIL in particular, because it does not cover project management or architecture, we have a very large quantity of potential process interactions that is nevertheless incomplete.

anchor:secondary-artifacts[]

====== Secondary artifacts, compounded by batch orientation
[quote, Jeff Gothelf, Lean UX]
We move away from heavily documented handoffs to a process that creates only the design artifacts we need to move the team’s learning forward.

The process handoffs also imply that artifacts (documents of various sorts, models, software, etc.) are being created and transferred in between teams, or at least between roles on the same team with some degree of formality.

Primary artifacts are executable software and any additional content intended directly for value delivery. Secondary artifacts are anything else.

An examination of the ITIL and COBIT process interactions shows that many of the artifacts are secondary concepts such as "plans," "designs," or "reports:"

* Design specifications (high level and detailed)
* Operation and use plan
* Performance reports
* Action plans
* Consideration and approval

and so on. (Note that actually executable artifacts are not included here.)

Again, artifacts do not create themselves. Hundreds of artifacts are implied in the process frameworks. Every artifact implies:

* Some template or known technique for performing it
* People trained in its creation and interpretation
* Some capability to store, version, and transmit it

Unstructured artifacts such as plans, designs, and reports in particular impose high cognitive load. As digital organizations automate their pipelines, it becomes essential to identify the key events and elements they may represent, so that they can be embedded into the automation layer.

Finally, even if a given process framework does not specifically call for waterfall, one can sometimes still see its legacy. For example:

* Calls for thorough project planning and estimation
* Cautions against "cutting corners"
* "Design specifications" moving through approval pipelines (and following a progression from general to detailed)

All of these tend to signal a large batch orientation, even in frameworks making some claim of supporting Agile.

Good system design is a complex process. We introduced xref:technical-debt-1[technical debt] in Chapter 3, and will revisit it in technical-debt[Chapter 12]. But the slow xref:feedback[feedback] signals resulting from the batch processes implied by some frameworks are unacceptable in current industry. This is in part why new approaches are being adopted.

====== Confusion of process definition

One final issue with the "process" frameworks is that, while they use the word "process" prominently, they are not aligned with Business Process Management best practices. cite:[Betz2011b]

All of these frameworks provide useful descriptions of major ongoing capabilities and functions that the large IT organization must perform. But in terms of our preceding discussion on process method, they in general are developed from the perspective of steady-state functions, as opposed to a value stream or defined process perspective.

This can be seen by looking above at ITIL, for example. A Business Process Management consultant would see the term "Capacity Management" and observe that it is not countable or event-driven. "How many Capacities did you do today?," might be the question.

The BPM community is clear that processes and countable and event-driven (see cite:[Sharp2009]). Naming them with a strong active verb is seen as essential. "True" IT processes therefore might include:

* Accept Demand
* Deliver Release
* Complete Change
* Resolve Incident
* Improve Service

.Author's view
****
In my experience the lack of countability throughout the IT frameworks' conception of "process" has caused confusion and lack of alignment with BPM professionals for years, and remains an ongoing problem.
****
